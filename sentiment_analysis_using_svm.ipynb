{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analysis using svm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1V1N7moUj8Ns28p0mddGiG622I680o3V9",
      "authorship_tag": "ABX9TyMk+cco7nciUgFc5KgLpzCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudevabhat0605/Amazon-Review-Sentiment-Analysis/blob/main/sentiment_analysis_using_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G6dvgtrWcZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7caa357c-077b-480a-dcbd-261f2cf3b430"
      },
      "source": [
        "!pip install scikit-learn\n",
        "!pip install vaderSentiment\n",
        "!pip install contractions\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vs = SentimentIntensityAnalyzer()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "import contractions\n",
        "import unicodedata\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/11/4d/378ab91284c2c3a06ab475b287721c09b7951d5ecb3edf4ffb0e1e7a568a/contractions-0.0.49-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 5.6MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 7.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85404 sha256=c460f8e3ae7a8c28f5a936eed6b401d2576cb51bec7d60b069ef9892a95ff4ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.49 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1dt57O7BihK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8a0224bb-0d9e-4cfc-ed15-90b185d0888e"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/major_p_datasets/AllProductReviews.csv\",nrows=5000)\n",
        "print(df.shape)\n",
        "df.ReviewTitle = df.ReviewTitle.str.replace('\\n', '')\n",
        "df.ReviewBody = df.ReviewBody.str.replace('\\n', '')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ReviewTitle</th>\n",
              "      <th>ReviewBody</th>\n",
              "      <th>ReviewStar</th>\n",
              "      <th>Product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Honest review of an edm music lover</td>\n",
              "      <td>No doubt it has a great bass and to a great ex...</td>\n",
              "      <td>3</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Unreliable earphones with high cost</td>\n",
              "      <td>This  earphones are unreliable, i bought it be...</td>\n",
              "      <td>1</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Really good and durable.</td>\n",
              "      <td>i bought itfor 999,I purchased it second time,...</td>\n",
              "      <td>4</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped working in just 14 days</td>\n",
              "      <td>Its sound quality is adorable. overall it was ...</td>\n",
              "      <td>1</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just Awesome Wireless Headphone under 1000...😉</td>\n",
              "      <td>Its Awesome... Good sound quality &amp; 8-9 hrs ba...</td>\n",
              "      <td>5</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      ReviewTitle  ...           Product\n",
              "0             Honest review of an edm music lover  ...  boAt Rockerz 255\n",
              "1             Unreliable earphones with high cost  ...  boAt Rockerz 255\n",
              "2                        Really good and durable.  ...  boAt Rockerz 255\n",
              "3                 stopped working in just 14 days  ...  boAt Rockerz 255\n",
              "4  Just Awesome Wireless Headphone under 1000...😉  ...  boAt Rockerz 255\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp06wRrk9jlw"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "len(stopword_list)\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def expand_contractions(text):\n",
        "    expanded_text = contractions.fix(text)\n",
        "    return expanded_text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRMBN9q5YO56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c4c2eb08-b1c8-4e3d-e441-5356a5c8799d"
      },
      "source": [
        "df.ReviewTitle\t= df.ReviewTitle.apply(lambda x:x.lower())\n",
        "df.ReviewBody = df.ReviewBody.apply(lambda x:x.lower())\n",
        "\n",
        "df.ReviewTitle = df.ReviewTitle.apply(remove_accented_chars)\n",
        "df.ReviewBody = df.ReviewBody.apply(remove_accented_chars)\n",
        "\n",
        "df.ReviewTitle = df.ReviewTitle.apply(expand_contractions)\n",
        "df.ReviewBody = df.ReviewBody.apply(expand_contractions)\n",
        "\n",
        "df.ReviewTitle = df.ReviewTitle.apply(remove_special_characters)\n",
        "df.ReviewBody = df.ReviewBody.apply(remove_special_characters)\n",
        "\n",
        "df.ReviewTitle = df.ReviewTitle.apply(remove_stopwords)\n",
        "df.ReviewBody = df.ReviewBody.apply(remove_stopwords)\n",
        "\n",
        "df['ReviewTitle_token'] = df.ReviewTitle.apply(lambda x: word_tokenize(x))\n",
        "df['ReviewBody_token'] = df.ReviewBody.apply(lambda x: word_tokenize(x))\n",
        "\n",
        "df['ReviewTitle_lem'] = df.ReviewTitle_token.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "df['ReviewBody_lem'] = df.ReviewBody_token.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "def listToString(s):  \n",
        "    \n",
        "    # initialize an empty string \n",
        "    str1 = \" \" \n",
        "    \n",
        "    # return string   \n",
        "    return (str1.join(s)) \n",
        "\n",
        "df['ReviewTitle_new'] = df.ReviewTitle_lem.apply(listToString)\n",
        "df['ReviewBody_new'] = df.ReviewBody_lem.apply(listToString)\n",
        "\n",
        "new_df = df[['ReviewTitle_new','ReviewBody_new','Product']]\n",
        "\n",
        "\n",
        "new_df.to_csv('AllProductReviews.csv',index=True,encoding='utf-8')\n",
        "new_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ReviewTitle_new</th>\n",
              "      <th>ReviewBody_new</th>\n",
              "      <th>Product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>honest review edm music lover</td>\n",
              "      <td>no doubt great bass great extent noise cancell...</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unreliable earphone high cost</td>\n",
              "      <td>earphone unreliable bought day meanwhile right...</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>really good durable</td>\n",
              "      <td>bought itfor purchased second time gifted firs...</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped working day</td>\n",
              "      <td>sound quality adorable overall good week stopp...</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>awesome wireless headphone</td>\n",
              "      <td>awesome good sound quality hr battery life waw...</td>\n",
              "      <td>boAt Rockerz 255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ReviewTitle_new  ...           Product\n",
              "0  honest review edm music lover  ...  boAt Rockerz 255\n",
              "1  unreliable earphone high cost  ...  boAt Rockerz 255\n",
              "2            really good durable  ...  boAt Rockerz 255\n",
              "3            stopped working day  ...  boAt Rockerz 255\n",
              "4     awesome wireless headphone  ...  boAt Rockerz 255\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "486up4QnI2nE"
      },
      "source": [
        "df['compound'] = df['ReviewBody'].apply(lambda x: vs.polarity_scores(x)['compound'])\n",
        "df['comp_score'] = df['compound'].apply(lambda c: 'positive' if c>=0 else 'negative' )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zogJvsTm14rc"
      },
      "source": [
        "x = new_df['ReviewBody_new'].values\n",
        "y = df['ReviewStar'].values\n",
        "x.astype(str)\n",
        "y.astype(str)\n",
        "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.1, random_state=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa-R5AlnegN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc53478-6dff-42f3-84b2-002741f11792"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "np.unique(y_train,return_counts=True)\n",
        "np.unique(y_test,return_counts=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4500,)\n",
            "(500,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1, 2, 3, 4, 5]), array([ 84,  34,  63, 129, 190]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RcPQm5qfNKK"
      },
      "source": [
        "vect = TfidfVectorizer()\n",
        "x_train = vect.fit_transform(x_train)\n",
        "x_test = vect.transform(x_test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRah1_hyDGsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0357506-f341-411e-a050-922c63a3c5d4"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "model = SVC( )\n",
        "model.fit(x_train,y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 5, 5, 1, 5, 1, 5, 5, 5, 5, 3, 4, 1, 1, 5, 4, 5, 4, 1, 5, 4, 4,\n",
              "       5, 4, 1, 4, 5, 1, 5, 5, 1, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 3, 5,\n",
              "       4, 1, 4, 5, 4, 5, 4, 1, 5, 5, 5, 1, 5, 5, 5, 5, 1, 5, 4, 5, 4, 4,\n",
              "       1, 4, 3, 5, 5, 3, 5, 5, 5, 5, 1, 1, 1, 4, 1, 3, 5, 4, 5, 5, 1, 5,\n",
              "       1, 5, 5, 4, 5, 5, 5, 5, 1, 5, 5, 4, 4, 4, 5, 5, 5, 1, 3, 5, 3, 1,\n",
              "       5, 1, 1, 1, 5, 5, 5, 1, 4, 5, 5, 5, 1, 5, 5, 4, 5, 1, 5, 1, 4, 5,\n",
              "       5, 4, 4, 5, 4, 4, 5, 5, 1, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 1,\n",
              "       1, 5, 5, 5, 5, 3, 5, 1, 5, 5, 5, 3, 5, 5, 1, 1, 5, 5, 5, 1, 5, 4,\n",
              "       1, 5, 2, 5, 5, 5, 5, 5, 5, 3, 5, 4, 1, 1, 1, 5, 4, 5, 1, 5, 4, 1,\n",
              "       5, 5, 5, 5, 5, 5, 1, 1, 5, 5, 5, 4, 5, 5, 5, 1, 5, 4, 5, 4, 5, 3,\n",
              "       1, 5, 1, 5, 1, 1, 5, 5, 5, 4, 4, 4, 5, 5, 1, 5, 5, 4, 4, 4, 5, 5,\n",
              "       5, 1, 1, 5, 5, 5, 5, 1, 4, 1, 4, 1, 1, 5, 3, 4, 5, 1, 1, 5, 5, 5,\n",
              "       5, 3, 5, 5, 5, 5, 4, 5, 1, 5, 5, 5, 1, 5, 5, 1, 4, 5, 5, 3, 4, 1,\n",
              "       5, 5, 5, 5, 1, 5, 5, 5, 3, 4, 1, 1, 4, 4, 5, 5, 5, 4, 5, 4, 1, 5,\n",
              "       5, 4, 5, 1, 5, 5, 4, 4, 1, 5, 4, 5, 5, 5, 5, 4, 4, 1, 5, 5, 5, 5,\n",
              "       4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 1, 5, 5, 4, 4, 5, 5, 4, 1,\n",
              "       5, 5, 4, 5, 5, 5, 5, 4, 1, 1, 4, 4, 5, 3, 1, 3, 4, 5, 5, 1, 1, 3,\n",
              "       5, 4, 5, 4, 4, 5, 1, 3, 5, 5, 5, 3, 1, 5, 1, 1, 1, 5, 4, 5, 5, 1,\n",
              "       5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 1, 5, 5, 3, 5,\n",
              "       5, 5, 1, 1, 5, 5, 1, 4, 5, 5, 5, 4, 1, 1, 5, 4, 1, 4, 5, 5, 1, 5,\n",
              "       5, 5, 1, 5, 4, 4, 5, 5, 4, 5, 5, 1, 5, 4, 1, 5, 5, 4, 5, 5, 5, 1,\n",
              "       1, 4, 4, 5, 1, 5, 5, 1, 5, 5, 5, 5, 3, 5, 3, 5, 5, 5, 4, 4, 1, 4,\n",
              "       5, 5, 5, 5, 3, 5, 5, 4, 5, 5, 1, 1, 4, 5, 5, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQK6GHqWcCJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac771c4-7ec3-4e6b-dfcc-4b0e02617d12"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 5, 4, 1, 3, 5, 5, 5, 3, 3, 3, 5, 1, 3, 5, 3, 5, 1, 5, 4, 4, 5,\n",
              "       4, 4, 1, 3, 4, 1, 5, 5, 1, 4, 5, 4, 5, 5, 4, 3, 4, 5, 4, 4, 2, 3,\n",
              "       2, 2, 4, 5, 1, 3, 4, 2, 4, 5, 5, 1, 4, 5, 5, 5, 1, 4, 3, 5, 5, 3,\n",
              "       1, 5, 5, 5, 5, 5, 3, 1, 4, 5, 1, 1, 1, 4, 1, 2, 4, 3, 5, 4, 2, 5,\n",
              "       1, 5, 5, 4, 4, 5, 4, 5, 3, 5, 3, 1, 4, 5, 4, 5, 5, 1, 2, 3, 1, 1,\n",
              "       5, 1, 3, 1, 3, 5, 3, 2, 1, 5, 5, 5, 3, 5, 5, 3, 5, 3, 5, 1, 4, 5,\n",
              "       5, 4, 3, 5, 5, 3, 4, 4, 1, 2, 5, 4, 5, 5, 5, 3, 5, 5, 4, 2, 5, 1,\n",
              "       1, 3, 5, 5, 5, 3, 2, 3, 4, 2, 4, 4, 5, 5, 1, 1, 5, 5, 4, 1, 5, 4,\n",
              "       5, 5, 2, 5, 4, 4, 5, 5, 5, 4, 4, 5, 1, 1, 3, 5, 4, 4, 2, 5, 4, 2,\n",
              "       3, 5, 4, 5, 5, 4, 2, 1, 4, 5, 4, 3, 5, 5, 1, 1, 4, 5, 4, 2, 3, 3,\n",
              "       1, 4, 1, 4, 2, 1, 4, 5, 5, 4, 4, 5, 5, 5, 1, 5, 4, 3, 4, 4, 4, 4,\n",
              "       5, 4, 2, 5, 5, 5, 5, 3, 4, 1, 3, 1, 1, 4, 1, 4, 5, 5, 1, 5, 1, 5,\n",
              "       5, 2, 4, 4, 5, 4, 5, 5, 1, 5, 5, 5, 1, 1, 5, 1, 3, 4, 5, 3, 3, 5,\n",
              "       1, 5, 5, 4, 4, 5, 5, 5, 4, 4, 1, 2, 1, 2, 5, 5, 4, 4, 5, 5, 1, 4,\n",
              "       5, 4, 5, 2, 5, 4, 4, 5, 1, 5, 4, 4, 4, 2, 5, 4, 3, 1, 4, 4, 5, 5,\n",
              "       4, 3, 3, 5, 3, 3, 1, 2, 4, 4, 4, 4, 4, 1, 5, 5, 4, 5, 5, 5, 5, 1,\n",
              "       4, 5, 5, 5, 5, 4, 4, 1, 1, 2, 4, 5, 4, 1, 1, 1, 5, 5, 4, 2, 1, 3,\n",
              "       5, 4, 4, 5, 4, 4, 1, 2, 5, 4, 5, 3, 1, 4, 1, 1, 1, 5, 5, 4, 5, 3,\n",
              "       5, 5, 5, 5, 4, 4, 5, 5, 5, 1, 4, 5, 3, 5, 5, 5, 2, 1, 5, 5, 3, 4,\n",
              "       4, 5, 3, 1, 4, 5, 3, 4, 4, 5, 4, 1, 2, 2, 5, 3, 1, 3, 5, 4, 1, 5,\n",
              "       5, 5, 1, 5, 3, 5, 5, 5, 3, 4, 3, 4, 5, 4, 2, 4, 5, 5, 4, 5, 4, 1,\n",
              "       5, 3, 4, 5, 1, 5, 3, 1, 5, 5, 5, 5, 3, 4, 3, 5, 5, 5, 1, 4, 3, 4,\n",
              "       5, 4, 5, 1, 2, 4, 5, 4, 1, 5, 1, 1, 4, 4, 2, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbFrxMQAgpjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295a0594-b18a-4eb5-827d-f188e99413fa"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "accuracy_score(y_pred,y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.542"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFG0Gud9hG5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b4baae-b217-4aba-b3cf-81a7218690ed"
      },
      "source": [
        "confusion_matrix(y_pred,y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 63,  16,  12,   3,   6],\n",
              "       [  0,   1,   0,   0,   0],\n",
              "       [  4,   6,   9,   3,   2],\n",
              "       [  9,   6,  25,  42,  26],\n",
              "       [  8,   5,  17,  81, 156]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjUoonAphSXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72eeb6d8-ff3d-4536-a87b-7c380ab917e2"
      },
      "source": [
        "print(classification_report(y_pred,y_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.75      0.63      0.68       100\n",
            "           2       0.03      1.00      0.06         1\n",
            "           3       0.14      0.38      0.21        24\n",
            "           4       0.33      0.39      0.35       108\n",
            "           5       0.82      0.58      0.68       267\n",
            "\n",
            "    accuracy                           0.54       500\n",
            "   macro avg       0.41      0.60      0.40       500\n",
            "weighted avg       0.67      0.54      0.59       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kS6uqiHiC2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c057b68a-a953-433d-df85-0c4e5d593b23"
      },
      "source": [
        "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.1, random_state=0)\n",
        "text_model = Pipeline([('vect',TfidfVectorizer( )),('model',SVC( ))])\n",
        "text_model.fit(x_train,y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('model',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVfokdD-0S1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ba186a-b5b5-402b-aa6b-0790356e68f7"
      },
      "source": [
        "y_pred = text_model.predict(x_test)\n",
        "y_pred"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 5, 5, 1, 5, 1, 5, 5, 5, 5, 3, 4, 1, 1, 5, 4, 5, 4, 1, 5, 4, 4,\n",
              "       5, 4, 1, 4, 5, 1, 5, 5, 1, 4, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 3, 5,\n",
              "       4, 1, 4, 5, 4, 5, 4, 1, 5, 5, 5, 1, 5, 5, 5, 5, 1, 5, 4, 5, 4, 4,\n",
              "       1, 4, 3, 5, 5, 3, 5, 5, 5, 5, 1, 1, 1, 4, 1, 3, 5, 4, 5, 5, 1, 5,\n",
              "       1, 5, 5, 4, 5, 5, 5, 5, 1, 5, 5, 4, 4, 4, 5, 5, 5, 1, 3, 5, 3, 1,\n",
              "       5, 1, 1, 1, 5, 5, 5, 1, 4, 5, 5, 5, 1, 5, 5, 4, 5, 1, 5, 1, 4, 5,\n",
              "       5, 4, 4, 5, 4, 4, 5, 5, 1, 4, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 5, 1,\n",
              "       1, 5, 5, 5, 5, 3, 5, 1, 5, 5, 5, 3, 5, 5, 1, 1, 5, 5, 5, 1, 5, 4,\n",
              "       1, 5, 2, 5, 5, 5, 5, 5, 5, 3, 5, 4, 1, 1, 1, 5, 4, 5, 1, 5, 4, 1,\n",
              "       5, 5, 5, 5, 5, 5, 1, 1, 5, 5, 5, 4, 5, 5, 5, 1, 5, 4, 5, 4, 5, 3,\n",
              "       1, 5, 1, 5, 1, 1, 5, 5, 5, 4, 4, 4, 5, 5, 1, 5, 5, 4, 4, 4, 5, 5,\n",
              "       5, 1, 1, 5, 5, 5, 5, 1, 4, 1, 4, 1, 1, 5, 3, 4, 5, 1, 1, 5, 5, 5,\n",
              "       5, 3, 5, 5, 5, 5, 4, 5, 1, 5, 5, 5, 1, 5, 5, 1, 4, 5, 5, 3, 4, 1,\n",
              "       5, 5, 5, 5, 1, 5, 5, 5, 3, 4, 1, 1, 4, 4, 5, 5, 5, 4, 5, 4, 1, 5,\n",
              "       5, 4, 5, 1, 5, 5, 4, 4, 1, 5, 4, 5, 5, 5, 5, 4, 4, 1, 5, 5, 5, 5,\n",
              "       4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 1, 5, 5, 4, 4, 5, 5, 4, 1,\n",
              "       5, 5, 4, 5, 5, 5, 5, 4, 1, 1, 4, 4, 5, 3, 1, 3, 4, 5, 5, 1, 1, 3,\n",
              "       5, 4, 5, 4, 4, 5, 1, 3, 5, 5, 5, 3, 1, 5, 1, 1, 1, 5, 4, 5, 5, 1,\n",
              "       5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 1, 5, 5, 3, 5,\n",
              "       5, 5, 1, 1, 5, 5, 1, 4, 5, 5, 5, 4, 1, 1, 5, 4, 1, 4, 5, 5, 1, 5,\n",
              "       5, 5, 1, 5, 4, 4, 5, 5, 4, 5, 5, 1, 5, 4, 1, 5, 5, 4, 5, 5, 5, 1,\n",
              "       1, 4, 4, 5, 1, 5, 5, 1, 5, 5, 5, 5, 3, 5, 3, 5, 5, 5, 4, 4, 1, 4,\n",
              "       5, 5, 5, 5, 3, 5, 5, 4, 5, 5, 1, 1, 4, 5, 5, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vohoafmd7Sb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9244145c-82ed-4720-deae-3882402bb27c"
      },
      "source": [
        "accuracy_score(y_pred,y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.542"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q59D28Oy7gwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38fac256-b445-4c64-d227-65b61db86b17"
      },
      "source": [
        "text = df['ReviewBody'][10]\n",
        "text_model.predict([text])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqmL2lD0zjWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26731e9c-10d4-4222-f11f-1b4232aa9cd4"
      },
      "source": [
        "!pip install joblib\n",
        "import pickle\n",
        "import joblib\n",
        "joblib.dump(text_model,'sentiment analysis')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sentiment analysis']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}